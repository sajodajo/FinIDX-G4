{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 5 - FINANCIAL TIME SERIES\n",
        "\n",
        "In this lab, you will be given two tasks:\n",
        "\n",
        "1. Explore and understand code fitting GARCH models to the log-returns of Bitcoin from 2019 to 2024.\n",
        "\n",
        "2. Choose a different financial index you might be interested in - for instance Amazon (AMZN), Tesla (TSLA), JPMorgan Chase (JPM), Goldman Sachs (GS), American Express (AMX), Capri Holdings (CPRI), Louis Vuitton group (LVMUY), InterContinental Hotel Group (IHG), Delta Airlines (DAL), or any other that you like. Use the code from the previous part to investigate the behaviour of the index. You can also pick the one you have been discussing within your group and start the analysis for the presentation."
      ],
      "metadata": {
        "id": "dNyw80Bze2XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 1 - BITCOIN\n",
        "\n",
        "In this lab you will analyze the evolution of the closing price of Bitcoin using volatility models. To apply them you first need to install the `arch` library. The data is retrieved from `yfinance` which should be already installed in your machine."
      ],
      "metadata": {
        "id": "moZAq7HBfiX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arch"
      ],
      "metadata": {
        "id": "qmoDWOsqrYIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOADING AND VISUALIZING THE DATA\n",
        "The following code downloads the closing prices of Bitcoin and computes the percentage log-returns and absolute log-returns (absolute values). It then creates the two time series plots. In the plot of the absolute values you also have a LOESS smoothing of the time series as well as a rolling estimate of the standard deviation of the absolute data.\n",
        "\n",
        " 1. Is there evidence of volatility clustering? Are there years where the evolution of the prices of Bitcoin was more stable?\n",
        "\n",
        " 2. Play with the window size of the rolling standard deviation estimate and the `frac` input of the LOESS smoothing. What happens when you vary these parameters?"
      ],
      "metadata": {
        "id": "z4-1PWA02p8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05hCVWPtafli"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Fetch Bitcoin data from Yahoo Finance (2019-2024)\n",
        "btc = yf.download(\"BTC-USD\", start=\"2019-01-01\", end=\"2025-01-01\", progress=False)\n",
        "\n",
        "# Compute percentage log-returns\n",
        "btc['Log_Return'] = np.log(btc['Close'] / btc['Close'].shift(1)) * 100\n",
        "btc.dropna(inplace=True)\n",
        "\n",
        "# Compute absolute log-returns\n",
        "btc['Abs_Log_Return'] = np.abs(btc['Log_Return'])\n",
        "\n",
        "# Compute rolling 60-day standard deviation\n",
        "btc['Rolling_Std'] = btc['Log_Return'].rolling(window=60).std()\n",
        "\n",
        "# LOESS smoothing (LOWESS from statsmodels)\n",
        "lowess = sm.nonparametric.lowess\n",
        "smoothed_abs = lowess(btc['Abs_Log_Return'], btc.index, frac=0.03)  # frac=0.03 controls smoothness\n",
        "\n",
        "# Plot time series\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 8), sharex=True)\n",
        "\n",
        "# Plot log-returns\n",
        "top_ax = axes[0]\n",
        "top_ax.plot(btc.index, btc['Log_Return'], color='blue', alpha=0.6, label='Log-Returns')\n",
        "top_ax.set_ylabel(\"Log-Returns (%)\")\n",
        "top_ax.set_title(\"Bitcoin Log-Returns (2019-2024)\")\n",
        "top_ax.legend()\n",
        "\n",
        "# Plot absolute log-returns with LOESS and rolling std\n",
        "bottom_ax = axes[1]\n",
        "bottom_ax.plot(btc.index, btc['Abs_Log_Return'], color='gray', alpha=0.5, label='Absolute Log-Returns')\n",
        "bottom_ax.plot(btc.index, btc['Rolling_Std'], color='red', label='60-Day Rolling Std')\n",
        "bottom_ax.plot(btc.index, smoothed_abs[:, 1], color='black', label='LOESS Smoothed Abs Returns')\n",
        "bottom_ax.set_ylabel(\"Absolute Log-Returns (%)\")\n",
        "bottom_ax.set_title(\"Absolute Log-Returns with LOESS and Rolling Std (2019-2024)\")\n",
        "bottom_ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FITTING A NORMAL DISTRIBUTION\n",
        "We now forget for a second that the data is a time-series and simply check if overall it follows a Normal distribution. To do this we compute the sample mean, variance, skewness and kurtosis. What do these values tell you?\n",
        "\n",
        "Furthermore, we plot the histogram of the data with the best Normal line. Does the assumption of Normality holds? To help you, I also report the p-value of a test of Normality (null hypothesis: the data follows a Normal distribution)."
      ],
      "metadata": {
        "id": "KM6zse_0f4Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, kurtosis, normaltest, norm\n",
        "\n",
        "# Compute statistics\n",
        "mean_return = np.mean(btc['Log_Return'])\n",
        "variance_return = np.var(btc['Log_Return'])\n",
        "skewness_return = skew(btc['Log_Return'])\n",
        "kurtosis_return = kurtosis(btc['Log_Return'])\n",
        "\n",
        "# Normality test (D'Agostino and Pearson’s test)\n",
        "stats_test, p_value = normaltest(btc['Log_Return'])\n",
        "normality_result = \"Normal\" if p_value > 0.05 else \"Not Normal\"\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Mean: {mean_return:.4f}\")\n",
        "print(f\"Variance: {variance_return:.4f}\")\n",
        "print(f\"Skewness: {skewness_return:.4f}\")\n",
        "print(f\"Kurtosis: {kurtosis_return:.4f}\")\n",
        "print(f\"Normality Test p-value: {p_value:.4f} ({normality_result})\")\n",
        "\n",
        "# Plot histogram with overlaid normal distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "count, bins, _ = plt.hist(btc['Log_Return'], bins=50, alpha=0.7, color='blue', edgecolor='black', density=True)\n",
        "\n",
        "# Compute normal distribution curve\n",
        "x = np.linspace(bins[0], bins[-1], 100)\n",
        "pdf = norm.pdf(x, mean_return, np.sqrt(variance_return))\n",
        "plt.plot(x, pdf, color='red', lw=2, label=f'Normal Dist (μ={mean_return:.2f}, σ²={variance_return:.2f})')\n",
        "\n",
        "plt.xlabel(\"Log-Returns (%)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Histogram of Bitcoin Log-Returns (2019-2024) with Normal Distribution\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d14wwKGcam8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FITTING A T-DISTRIBUTION\n",
        "We now repeat the previous step, but now assuming a T-distribution. Interpret the results and parameter estimates. Does a T-distribution provide a better description of the data?"
      ],
      "metadata": {
        "id": "mqehkWDbgHoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import t\n",
        "\n",
        "# Fit a t-distribution to the log-returns\n",
        "params = t.fit(btc['Log_Return'])  # Direct fitting\n",
        "\n",
        "# Extract fitted parameters\n",
        "df_t, loc_t, scale_t = params\n",
        "\n",
        "# Compute variance and kurtosis of the fitted t-distribution\n",
        "variance_t = (df_t / (df_t - 2)) * (scale_t ** 2) if df_t > 2 else np.nan\n",
        "kurtosis_t = (6 / (df_t - 4)) if df_t > 4 else np.inf  # Infinite for df <= 4\n",
        "\n",
        "# Print fitted parameters and statistics\n",
        "print(f\"Fitted t-distribution parameters:\")\n",
        "print(f\"Degrees of Freedom: {df_t:.4f}\")\n",
        "print(f\"Location: {loc_t:.4f}\")\n",
        "print(f\"Scale: {scale_t:.4f}\")\n",
        "print(f\"Variance: {variance_t:.4f}\")\n",
        "print(f\"Kurtosis: {'Infinite' if np.isinf(kurtosis_t) else f'{kurtosis_t:.4f}'}\")\n",
        "\n",
        "# Plot histogram with normal and t-distribution curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "count, bins, _ = plt.hist(btc['Log_Return'], bins=50, alpha=0.7, color='blue', edgecolor='black', density=True)\n",
        "\n",
        "# Compute normal and t-distribution curves\n",
        "x = np.linspace(bins[0], bins[-1], 100)\n",
        "pdf_norm = norm.pdf(x, np.mean(btc['Log_Return']), np.std(btc['Log_Return']))\n",
        "pdf_t = t.pdf(x, df_t, loc=loc_t, scale=scale_t)\n",
        "\n",
        "plt.plot(x, pdf_norm, color='red', lw=2, label=f'Normal Dist (μ={np.mean(btc[\"Log_Return\"]):.2f}, σ²={np.var(btc[\"Log_Return\"]):.2f})')\n",
        "plt.plot(x, pdf_t, color='green', lw=2, label=f't-Dist (df={df_t:.2f}, scale={scale_t:.2f})')\n",
        "\n",
        "plt.xlabel(\"Log-Returns (%)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Histogram of Bitcoin Log-Returns with Normal and t-Distributions\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T9v0F9gVasom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FITTING A GARCH MODEL\n",
        "We now move on to modeling. In the case of AR + GARCH models we could of course do some grid search of the best combination of parameters. However, usually the number of models that are worth considering is small and can be explored manually.\n",
        "\n",
        "Here I chose for Bitcoin an AR(1) + GARCH(1,1) model with a t-distribution for the errors. Look at the output and intepret the parameter estimates.\n",
        "\n",
        "Also I suggest you try different models.\n",
        "\n",
        " - For instance set `dist = 'normal'` which would assume a Normal distribution for the errors;\n",
        "\n",
        " - Try considering a GJR-GARCH(1,1,1) model by setting `o = 1`;\n",
        "\n",
        " - Look at ARCH models setting `q=0` and varying the input parameter `p`.\n",
        "\n",
        " The code also produces the standard `arch` output for you."
      ],
      "metadata": {
        "id": "bJGRFXymgbK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from arch import arch_model\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Prepare the data\n",
        "returns = btc['Log_Return'].dropna()\n",
        "\n",
        "# Fit GARCH(1,1) model with t-distributed errors\n",
        "garch_model = arch_model(returns, mean = 'AR', lags= 1, vol='Garch', p=1, o=0, q=1, dist='t')\n",
        "garch_fit = garch_model.fit(disp='off')\n",
        "print(\"\\nModel Summary:\")\n",
        "print(garch_fit.summary())\n",
        "\n",
        "# Plot GJR-GARCH model results\n",
        "fig = garch_fit.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PRxZWMKma5Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VISUALIZING VOLATILITY\n",
        "\n",
        "The following code creates two plots that are commonly created in finance to visualize the volatility of financial data.\n",
        "\n",
        "  1. The log-return series with a ribbon of +/- 2 standard deviations (dynamic)\n",
        "\n",
        "  2. The absolute log-returns with the standard deviation (also dynamic)"
      ],
      "metadata": {
        "id": "oLsrspAphUCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the conditional volatility (sigma_t) and standardized residuals\n",
        "conditional_volatility = garch_fit.conditional_volatility\n",
        "absolute_log_returns = np.abs(returns)\n",
        "\n",
        "# Combined plot with two subplots stacked vertically\n",
        "fig, axs = plt.subplots(2, 1, figsize=(14, 12))\n",
        "\n",
        "# Plot 1: Log returns with shaded ±2 conditional standard deviations\n",
        "axs[0].plot(returns, label='Log Returns', color='blue')\n",
        "upper_bound = 2 * conditional_volatility\n",
        "lower_bound = -2 * conditional_volatility\n",
        "axs[0].fill_between(btc.index, lower_bound, upper_bound, color='red', alpha=0.2, label='±2 Conditional Std Dev')\n",
        "axs[0].set_title('Log Returns with Conditional Std Deviations (Shaded ±2σ)')\n",
        "axs[0].set_xlabel('Time')\n",
        "axs[0].set_ylabel('Log Returns / Volatility')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot 2: Conditional Std Dev vs. Absolute Log Returns\n",
        "axs[1].plot(absolute_log_returns, label='|Log Returns|', color='gray', alpha=0.2)\n",
        "axs[1].plot(conditional_volatility, label='Conditional Std Dev (GARCH)', color='red')\n",
        "axs[1].set_title('Conditional Std Dev vs Absolute Log Returns')\n",
        "axs[1].set_xlabel('Time')\n",
        "axs[1].set_ylabel('Values')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Adjust layout and save the figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t5bgamZgblO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESIDUAL ANALYSIS\n",
        "\n",
        "Is the model appropriate for the data? The following code constructs the visualizations we have been using throughout this course to see if there are still patterns in the data that our model did not describe. Interpret all the plots below."
      ],
      "metadata": {
        "id": "ZVBXIsPrhn3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from scipy.stats import t, probplot\n",
        "\n",
        "# Compute standardized residuals\n",
        "std_residuals = garch_fit.resid / garch_fit.conditional_volatility\n",
        "std_residuals = std_residuals.dropna()\n",
        "\n",
        "# Set up 2x2 residual plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 1. Standardized residual time series\n",
        "sns.lineplot(x=std_residuals.index, y=std_residuals, ax=axes[0, 0], color='blue')\n",
        "axes[0, 0].set_title(\"Standardized Residuals Time Series\")\n",
        "axes[0, 0].set_ylabel(\"Standardized Residuals\")\n",
        "axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. ACF of standardized residuals\n",
        "plot_acf(std_residuals, ax=axes[0, 1], lags=40)\n",
        "axes[0, 1].set_title(\"ACF of Standardized Residuals\")\n",
        "\n",
        "# 3. ACF of squared standardized residuals\n",
        "plot_acf(std_residuals**2, ax=axes[1, 0], lags=40)\n",
        "axes[1, 0].set_title(\"ACF of Squared Standardized Residuals\")\n",
        "\n",
        "# 4. QQ-plot with estimated t-distribution\n",
        "df_t, loc_t, scale_t = t.fit(std_residuals)\n",
        "x = np.linspace(min(std_residuals), max(std_residuals), 100)\n",
        "pdf_t = t.pdf(x, df_t, loc=loc_t, scale=scale_t)\n",
        "\n",
        "axes[1, 1].hist(std_residuals, bins=50, density=True, alpha=0.6, color='blue', edgecolor='black', label=\"Residuals\")\n",
        "axes[1, 1].plot(x, pdf_t, color='red', lw=2, label=f't-Dist (df={df_t:.2f})')\n",
        "axes[1, 1].set_title(\"Histogram of Standardized Residuals with t-Distribution\")\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8z_D0ifseFgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RISK MODELING - VaR ESTIMATES\n",
        "\n",
        "Let's assume once more that the data is not a time series and let's compute non-dynamic estimates of the Value-at-Risk. The following code does a comprehensive of VaR.\n",
        "\n",
        "  1. It computes VaR for many possible levels (x-axis of the plot)\n",
        "  2. It considers the three approaches we have seen in class - historical; Normal distribution; T-distribution\n",
        "  3. It plots the VaR (y-axis) of the three methods for different levels.\n",
        "\n",
        "Of course in reality, it would be preferable to only use the T-distribution estimate, but here I show you all to compare the results."
      ],
      "metadata": {
        "id": "HBwcIGMCn3Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "returns = btc['Log_Return'].dropna()\n",
        "\n",
        "# Confidence levels for VaR\n",
        "confidence_levels = np.linspace(0.975, 0.9999, 100)\n",
        "\n",
        "# Compute VaR for different confidence levels\n",
        "VaR_hist = [np.percentile(returns, (1 - alpha) * 100) for alpha in confidence_levels]\n",
        "VaR_norm = [norm.ppf(1 - alpha, loc=np.mean(returns), scale=np.std(returns)) for alpha in confidence_levels]\n",
        "df_t, loc_t, scale_t = t.fit(returns)\n",
        "VaR_t = [loc_t + scale_t * t.ppf(1 - alpha, df_t) for alpha in confidence_levels]\n",
        "\n",
        "# Plot VaR estimates as a function of confidence level\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(confidence_levels * 100, VaR_hist, label='Historical VaR', linestyle='dashed', color='black')\n",
        "plt.plot(confidence_levels * 100, VaR_norm, label='Normal VaR', linestyle='dotted', color='red')\n",
        "plt.plot(confidence_levels * 100, VaR_t, label='t-Distribution VaR', linestyle='solid', color='green')\n",
        "\n",
        "plt.xlabel(\"Confidence Level (%)\")\n",
        "plt.ylabel(\"Log-Returns\")\n",
        "plt.title(\"Bitcoin VaR Estimates Across Confidence Levels\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O5UIFtZYkEL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RISK MANAGEMENT - EXPECTED SHORTFALL\n",
        "The following code creates the same plot as above, but now for ES."
      ],
      "metadata": {
        "id": "iep8N4WIoV2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Expected Shortfall (ES) using the proper formulas\n",
        "mean_return, std_return = returns.mean(), returns.std()\n",
        "phi_norm = norm.pdf(norm.ppf(confidence_levels))\n",
        "ES_norm = mean_return - std_return * (phi_norm / (1 - confidence_levels))\n",
        "\n",
        "# Compute t-Distribution Expected Shortfall\n",
        "t_alpha = t.ppf(confidence_levels, df_t)\n",
        "t_pdf_alpha = t.pdf(t_alpha, df_t)\n",
        "ES_t = loc_t - scale_t * (t_pdf_alpha / (1 - confidence_levels)) * (df_t + t_alpha**2) / (df_t - 1)\n",
        "\n",
        "# Compute Historical Expected Shortfall (ES) directly from data\n",
        "ES_hist = [returns[returns <= VaR_hist[i]].mean() for i in range(len(confidence_levels))]\n",
        "\n",
        "# Plot Expected Shortfall (ES) estimates as a function of confidence level\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(confidence_levels * 100, ES_hist, label='Historical ES', linestyle='dashed', color='black')\n",
        "plt.plot(confidence_levels * 100, ES_norm, label='Normal ES', linestyle='dotted', color='red')\n",
        "plt.plot(confidence_levels * 100, ES_t, label='t-Distribution ES', linestyle='solid', color='green')\n",
        "plt.xlabel(\"Confidence Level (%)\")\n",
        "plt.ylabel(\"Log-Returns\")\n",
        "plt.title(\"Bitcoin Expected Shortfall (ES) Estimates Across Confidence Levels\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xLPfKzVpoWHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DYNAMIC RISK MODELING\n",
        "\n",
        "As a final step, we compute VaR estimates for our historical data in a dynamic fashion using our GARCH model. The following code creates a plot of the dynamic VaR at each time point. Notice that in this plot we show the negative log-returns as customary (simply positive values are now losses)."
      ],
      "metadata": {
        "id": "WsSdEzX_oh61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract conditional volatility and standardized residuals\n",
        "std_residuals = garch_fit.resid / garch_fit.conditional_volatility\n",
        "std_residuals = std_residuals.dropna()\n",
        "cond_volatility = garch_fit.conditional_volatility.dropna()\n",
        "\n",
        "# Fit t-distribution to standardized residuals\n",
        "df_t, loc_t, scale_t = t.fit(std_residuals)\n",
        "\n",
        "# Compute dynamic VaR at 95% and 99%\n",
        "VaR_95 = -scale_t * t.ppf(0.05, df_t) * cond_volatility\n",
        "VaR_99 = -scale_t * t.ppf(0.01, df_t) * cond_volatility\n",
        "\n",
        "# Plot time series of negative log-returns with dynamic VaR\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(-returns, label=\"Negative Log-Returns\", color='blue', alpha=0.6)\n",
        "plt.plot(VaR_95, label=\"95% Dynamic VaR\", linestyle='dashed', color='red')\n",
        "plt.plot(VaR_99, label=\"99% Dynamic VaR\", linestyle='solid', color='black')\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Log-Returns\")\n",
        "plt.title(\"Bitcoin Negative Log-Returns with Dynamic VaR Estimates\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eeeQtZsNktx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2 - CHOOSE YOUR FINANCIAL INDEX!\n",
        "\n",
        "You are now free to investigate any time series that you like. You can compare the results with the ones we observed with bitcoin."
      ],
      "metadata": {
        "id": "sVfCaCH06Vu_"
      }
    }
  ]
}